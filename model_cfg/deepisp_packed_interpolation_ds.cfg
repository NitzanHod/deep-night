[net]
width=416
height=416
channels=4


#replacing the interpolation
[convolutional]
batch_normalize=1
filters=12
size=3
stride=1
pad=1
orientation=depthwise
activation=relu

[convolutional]
batch_normalize=1
filters=12
orientation=pointwise
activation=relu

[pixel_shuffle]
upscale_factor = 2
output = 0
to_be_routed = 1


#low level first block

[convolutional]
batch_normalize=1
filters=13
size=3
stride=1
pad=1
orientation=depthwise
activation=relu

[convolutional]
batch_normalize=1
filters=13
orientation=pointwise
activation=relu
to_be_routed=2

[route]
layers =-3
to_be_routed=1

[convolutional]
batch_normalize=1
filters=3
size=3
stride=1
pad=1
orientation=depthwise
activation=tanh

[convolutional]
batch_normalize=1
filters=3
orientation=pointwise
activation=tanh
to_be_routed=2

[shortcut]
layers = -3
to_be_routed=2

#1st low level core block

[route]
layers =-5,-1

[convolutional]
batch_normalize=1
filters=13
size=3
stride=1
pad=1
orientation=depthwise
activation=relu

[convolutional]
batch_normalize=1
filters=13
orientation=pointwise
activation=relu
to_be_routed=2

[route]
layers =-8,-4

[convolutional]
batch_normalize=1
filters=3
size=3
stride=1
pad=1
orientation=depthwise
activation=tanh

[convolutional]
batch_normalize=1
filters=3
orientation=pointwise
activation=tanh
to_be_routed=2

[shortcut]
layers = -8
to_be_routed=2

# 2nd low level core block
[route]
layers =-5,-1
[convolutional]
batch_normalize=1
filters=13
size=3
stride=1
pad=1
orientation=depthwise
activation=relu

[convolutional]
batch_normalize=1
filters=13
orientation=pointwise
activation=relu
to_be_routed=2

[route]
layers =-8,-4

[convolutional]
batch_normalize=1
filters=3
size=3
stride=1
pad=1
orientation=depthwise
activation=tanh

[convolutional]
batch_normalize=1
filters=3
orientation=pointwise
activation=tanh
to_be_routed=2

[shortcut]
layers = -8
to_be_routed=2

#high level part

# high level core
[route]
layers =-5
[convolutional]
batch_normalize=1
filters=64
size=3
stride=2
pad=1
orientation=depthwise
activation=relu
[convolutional]
batch_normalize=1
filters=64
orientation=pointwise
activation=relu

[maxpool]
size=2
stride=2

# another high level core

[convolutional]
batch_normalize=1
filters=64
size=3
stride=2
pad=1
orientation=depthwise
activation=relu

[convolutional]
batch_normalize=1
filters=64
orientation=pointwise
activation=relu

[maxpool]
size=2
stride=2

# ending high level
# global average pooling to 1X1X64

[mean]

#fully connected layer
[linear]
out_channels=30
to_be_routed=1

# layers = -1, - (2+ 3*N_high + 2) = -1*(3*N_high+4)
# here N_high is 2 so we get -10
[quadratic_transform]
layers=-1,-10
