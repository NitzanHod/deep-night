[net]
width=416
height=416
channels=3


[bilinear_interpolation]
to_be_routed=1

#low level first block

[convolutional]
batch_normalize=1
filters=13
size=3
stride=1
pad=1
orientation=regular
activation=relu
to_be_routed=2

[route]
layers =-2
to_be_routed=1

[convolutional]
batch_normalize=1
filters=3
size=3
stride=1
pad=1
orientation=regular
activation=tanh
to_be_routed=2

[shortcut]
from = -2
to_be_routed=2

#1st low level core block

[route]
layers =-3,-1

[convolutional]
batch_normalize=1
filters=13
size=3
stride=1
pad=1
orientation=regular
activation=relu
to_be_routed=2

[route]
layers =-6,-3

[convolutional]
batch_normalize=1
filters=3
size=3
stride=1
pad=1
orientation=regular
activation=tanh
to_be_routed=2

[shortcut]
from = -6
to_be_routed=2

# 2nd low level core block
[route]
layers =-3,-1

[convolutional]
batch_normalize=1
filters=13
size=3
stride=1
pad=1
orientation=regular
activation=relu
to_be_routed=2

[route]
layers =-6,-3

[convolutional]
batch_normalize=1
filters=3
size=3
stride=1
pad=1
orientation=regular
activation=tanh
to_be_routed=2

[shortcut]
from = -6
to_be_routed=2

#high level part

# high level core
[route]
layers =-4
[convolutional]
batch_normalize=1
filters=64
size=3
stride=2
pad=1
orientation=regular
activation=relu

[maxpool]
size=2
stride=2

# another high level core

[convolutional]
batch_normalize=1
filters=64
size=3
stride=2
pad=1
orientation=regular
activation=relu

[maxpool]
size=2
stride=2

# ending high level
# global average pooling to 1X1X64

[mean]

#fully connected layer
[linear]
out_channels=30
to_be_routed=1

# layers = -1, - (2+ 3*N_high + 2) = -1*(3*N_high+4)
# here N_high is 2 so we get -10
[quadratic_transform]
layers=-1,-8

