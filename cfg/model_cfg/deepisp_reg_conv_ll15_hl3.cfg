[net]
width=416
height=416
channels=4


#replacing the interpolation

[convolutional]
batch_normalize=1
filters=12
size=3
stride=1
pad=1
orientation=regular
activation=relu
to_be_routed=2


#low level first block - unique

[convolutional]
batch_normalize=1
filters=4
size=3
stride=1
pad=1
orientation=regular
activation=relu
to_be_routed=2

[route]
layers =-2
to_be_routed=1

[convolutional]
batch_normalize=1
filters=12
size=3
stride=1
pad=1
orientation=regular
activation=tanh
to_be_routed=2

[shortcut]
layers = -2
to_be_routed=2

#2nd low level core block

[route]
layers =-3,-1

[convolutional]
batch_normalize=1
filters=4
size=3
stride=1
pad=1
orientation=regular
activation=relu
to_be_routed=2

[route]
layers =-6,-3

[convolutional]
batch_normalize=1
filters=12
size=3
stride=1
pad=1
orientation=regular
activation=tanh
to_be_routed=2

[shortcut]
layers = -6
to_be_routed=2

#3rd low level core block

[route]
layers =-3,-1

[convolutional]
batch_normalize=1
filters=4
size=3
stride=1
pad=1
orientation=regular
activation=relu
to_be_routed=2

[route]
layers =-6,-3

[convolutional]
batch_normalize=1
filters=12
size=3
stride=1
pad=1
orientation=regular
activation=tanh
to_be_routed=2

[shortcut]
layers = -6
to_be_routed=2

#4th low level core block

[route]
layers =-3,-1

[convolutional]
batch_normalize=1
filters=4
size=3
stride=1
pad=1
orientation=regular
activation=relu
to_be_routed=2

[route]
layers =-6,-3

[convolutional]
batch_normalize=1
filters=12
size=3
stride=1
pad=1
orientation=regular
activation=tanh
to_be_routed=2

[shortcut]
layers = -6
to_be_routed=2

#5th low level core block

[route]
layers =-3,-1

[convolutional]
batch_normalize=1
filters=4
size=3
stride=1
pad=1
orientation=regular
activation=relu
to_be_routed=2

[route]
layers =-6,-3

[convolutional]
batch_normalize=1
filters=12
size=3
stride=1
pad=1
orientation=regular
activation=tanh
to_be_routed=2

[shortcut]
layers = -6
to_be_routed=2

#6th low level core block

[route]
layers =-3,-1

[convolutional]
batch_normalize=1
filters=4
size=3
stride=1
pad=1
orientation=regular
activation=relu
to_be_routed=2

[route]
layers =-6,-3

[convolutional]
batch_normalize=1
filters=12
size=3
stride=1
pad=1
orientation=regular
activation=tanh
to_be_routed=2

[shortcut]
layers = -6
to_be_routed=2

#7th low level core block

[route]
layers =-3,-1

[convolutional]
batch_normalize=1
filters=4
size=3
stride=1
pad=1
orientation=regular
activation=relu
to_be_routed=2

[route]
layers =-6,-3

[convolutional]
batch_normalize=1
filters=12
size=3
stride=1
pad=1
orientation=regular
activation=tanh
to_be_routed=2

[shortcut]
layers = -6
to_be_routed=2

#8th low level core block

[route]
layers =-3,-1

[convolutional]
batch_normalize=1
filters=4
size=3
stride=1
pad=1
orientation=regular
activation=relu
to_be_routed=2

[route]
layers =-6,-3

[convolutional]
batch_normalize=1
filters=12
size=3
stride=1
pad=1
orientation=regular
activation=tanh
to_be_routed=2

[shortcut]
layers = -6
to_be_routed=2

#9th low level core block

[route]
layers =-3,-1

[convolutional]
batch_normalize=1
filters=4
size=3
stride=1
pad=1
orientation=regular
activation=relu
to_be_routed=2

[route]
layers =-6,-3

[convolutional]
batch_normalize=1
filters=12
size=3
stride=1
pad=1
orientation=regular
activation=tanh
to_be_routed=2

[shortcut]
layers = -6
to_be_routed=2

#10th low level core block

[route]
layers =-3,-1

[convolutional]
batch_normalize=1
filters=4
size=3
stride=1
pad=1
orientation=regular
activation=relu
to_be_routed=2

[route]
layers =-6,-3

[convolutional]
batch_normalize=1
filters=12
size=3
stride=1
pad=1
orientation=regular
activation=tanh
to_be_routed=2

[shortcut]
layers = -6
to_be_routed=2

#11th low level core block

[route]
layers =-3,-1

[convolutional]
batch_normalize=1
filters=4
size=3
stride=1
pad=1
orientation=regular
activation=relu
to_be_routed=2

[route]
layers =-6,-3

[convolutional]
batch_normalize=1
filters=12
size=3
stride=1
pad=1
orientation=regular
activation=tanh
to_be_routed=2

[shortcut]
layers = -6
to_be_routed=2

#12th low level core block

[route]
layers =-3,-1

[convolutional]
batch_normalize=1
filters=4
size=3
stride=1
pad=1
orientation=regular
activation=relu
to_be_routed=2

[route]
layers =-6,-3

[convolutional]
batch_normalize=1
filters=12
size=3
stride=1
pad=1
orientation=regular
activation=tanh
to_be_routed=2

[shortcut]
layers = -6
to_be_routed=2

#13th low level core block

[route]
layers =-3,-1

[convolutional]
batch_normalize=1
filters=4
size=3
stride=1
pad=1
orientation=regular
activation=relu
to_be_routed=2

[route]
layers =-6,-3

[convolutional]
batch_normalize=1
filters=12
size=3
stride=1
pad=1
orientation=regular
activation=tanh
to_be_routed=2

[shortcut]
layers = -6
to_be_routed=2

#14th low level core block

[route]
layers =-3,-1

[convolutional]
batch_normalize=1
filters=4
size=3
stride=1
pad=1
orientation=regular
activation=relu
to_be_routed=2

[route]
layers =-6,-3

[convolutional]
batch_normalize=1
filters=12
size=3
stride=1
pad=1
orientation=regular
activation=tanh
to_be_routed=2

[shortcut]
layers = -6
to_be_routed=2

# 15th low level core block
[route]
layers =-3,-1

[convolutional]
batch_normalize=1
filters=4
size=3
stride=1
pad=1
orientation=regular
activation=relu
to_be_routed=2

[route]
layers =-6,-3

[convolutional]
batch_normalize=1
filters=12
size=3
stride=1
pad=1
orientation=regular
activation=tanh
to_be_routed=2

[shortcut]
layers = -6
to_be_routed=2

[pixel_shuffle]
upscale_factor = 2
output = 0

#high level part
[route]
layers =-5

# 1st high level core

[convolutional]
batch_normalize=1
filters=64
size=3
stride=2
pad=1
orientation=regular
activation=relu

[maxpool]
size=2
stride=2

# 2nd high level core

[convolutional]
batch_normalize=1
filters=64
size=3
stride=2
pad=1
orientation=regular
activation=relu

[maxpool]
size=2
stride=2

# 3rd high level core

[convolutional]
batch_normalize=1
filters=64
size=3
stride=2
pad=1
orientation=regular
activation=relu

[maxpool]
size=2
stride=2

# ending high level
# global average pooling to 1X1X64

[mean]

#fully connected layer
[linear]
out_channels=30
to_be_routed=1

# layers = -1, - (2+ 3*N_high + 2) = -1*(3*N_high+4)
# here N_high is 2 so we get -10
[quadratic_transform]
layers=-1,-10

